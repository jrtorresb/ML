# -*- coding: utf-8 -*-
"""Data_Analysis_COURSERA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QzeOcJCzfEg36Pdnmfzki-uIoDsFd0s1
"""

import matplotlib.pyplot as plt
plt.style.use('ggplot')

import pandas as pd

url='https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'

df=pd.read_csv(url, header=None)

df.head()

df.tail()

# create headers list
headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]

df.columns=headers

df.head()

df.dtypes

df.describe()

df.describe(include="all")

df.info()

# we can drop missing values along the column "price" as follows 
# df.dropna(subset=["price"], axis=0)

# Save
# df.to_csv("automobile.csv", index=False)

"""<p>
You can select the columns of a data frame by indicating the name of  each column, for example, you can select the three columns as follows:
</p>
<p>
    <code>dataframe[[' column 1 ',column 2', 'column 3']]</code>
</p>
<p>
Where "column" is the name of the column, you can apply the method  ".describe()" to get the statistics of those columns as follows:
</p>
<p>
    <code>dataframe[[' column 1 ',column 2', 'column 3'] ].describe()</code>
</p>

Apply the  method to ".describe()" to the columns 'length' and 'compression-ratio'.
"""

df.columns.values.tolist()

"""# DATA PRE-PROCESSING"""

df.isnull().sum()

# df["symboling"]=df["symboling"]+1

df.shape

"""## Missing Values"""

'''
dataframes.dropna()
axis=0 drops the entire row
axis=1 drops the entire column


df.dropna(axis=0): drops all rows that contain a nan

'''
# si te enfocas en la columna "price":
df.dropna(subset=["price"], axis=0, inplace=True)
# es equivalente a: df=df.dropna(subset=["price"], axis=0)

# si la haces as√≠: df.dropna(subset=["price"], axis=0) no cambias el dataframe

df.shape

"""## Replace Missing Values"""

'''
dataframe.replace(missing_value, new_value):
Primero calcula la media:
mean= df ["normalized-losses"].mean()
df["normalized-losses"].replace(np.nan, mean)



import numpy as np

# replace "?" to NaN
df.replace("?", np.nan, inplace = True)
df.head(5)


missing_data = df.isnull()
missing_data.head(5)

'''

missing_data = df.isnull()
missing_data.head(5)

for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

"""## Data Formatting"""

# Applying calculations to an entire column
df["city-mpg"]=235/df["city-mpg"]
df.head()

# Renombrar columna:
df.rename(columns={"city-mpg": "city-L/100KM"}, inplace=True)
df.head()

"""## Incorrect data types



*   Objects: "Hello" string
*   Int: 1,3,5       int
*   Float64: 2.123   float
"""

df.dtypes

# convert:
# df.astype()
df["price"] = df["price"].astype("int64")
df.dtypes

df["price"].values.tolist()

df["price"]=df["price"].replace("?", "0")

df["price"].values.tolist()

df.dtypes

df["price"] = df["price"].astype("int")

"""## Data Normalization"""

# MIN-MAX
df["length"]=(df["length"]-df["length"].min())/(df["length"].max()-df["length"].min())

df.head()

# Normalization by mean a std:
# df["length"]=(df["length"]-df["length"].mean())/df["length"].std()

"""<h4>Calculate the average of the column </h4>"""

import numpy as np

df.replace("?", np.nan, inplace = True)
df.head(5)


avg_norm_loss = df["normalized-losses"].astype("float").mean(axis=0)
print("Average of normalized-losses:", avg_norm_loss)
df["normalized-losses"].replace(np.nan, avg_norm_loss, inplace=True)


avg_bore=df['bore'].astype('float').mean(axis=0)
print("Average of bore:", avg_bore)
df["bore"].replace(np.nan, avg_bore, inplace=True)

df['num-of-doors'].value_counts()

df['num-of-doors'].value_counts().idxmax()

"""## Binning: Grouping of values into "bins"
Crear intervalos de clases: Low, Medium High
"""

import numpy as np
bins=np.linspace(min(df["price"]), max(df["price"]), 4)
group_names=["Low", "Medium", "High"]
df["price-binned"]=pd.cut(df["price"], bins, labels=group_names, include_lowest=True)

df.head()

df["price"].plot(kind="hist")

import matplotlib.pyplot as plt
plt.bar(group_names, df["price-binned"].value_counts())

# set x/y labels and plot title
plt.xlabel("horsepower")
plt.ylabel("count")
plt.title("horsepower bins")

"""## Turning Categorical Variables into Quantitative"""

# Convert categorical variables to dummy variables (o or 1)
# pandas.get_dummies()
pd.get_dummies(df["fuel-type"])

df.head()

df=pd.concat([df,pd.get_dummies(df["fuel-type"])], axis=1)
df.head()

# drop original column "fuel-type" from "df"
df.drop("fuel-type", axis = 1, inplace=True)
df.head()

"""## Exploratory Data Analysis (EDA)"""

df.describe()

# Agrupando CATEGORIAS
drive_wheels_counts=df["drive-wheels"].value_counts()
drive_wheels_counts

drive_wheels_counts.idxmax()

drive_wheels_counts.idxmin()

drive_wheels_counts.rename(columns={"drive-wheels": "value_couonts"}, inplace=True)
drive_wheels_counts

drive_wheels_counts.index.name="drive-wheels"
drive_wheels_counts

# BOXPLOT
import seaborn as sns
sns.boxplot(x="drive-wheels", y="price", data=df)

# SCATTER
y=df["engine-size"]
x=df["price"]

plt.scatter(x,y)
plt.title("Scatterplot of Engine Size vs Price")
plt.xlabel("Engine Size")
plt.ylabel("Price")
plt.show()

# GROUP BY
# dataframe.Groupby()

# Applied on categorical variables
# Group data into categories
# Single or multiple variables

# df[['price','body-style']].groupby(['body-style'],as_index= False).mean()

df_test=df[['drive-wheels','body-style','price']]
df_grp=df_test.groupby(["drive-wheels", "body-style"], as_index=False).mean()
df_grp

# Correlation
sns.regplot(x='engine-size', y="price", data=df)

sns.regplot(x='highway-mpg', y="price", data=df)
plt.show()

#sns.regplot(x='peak-rpm', y="price", data=df)
#plt.show()

pd.DataFrame(df.corr())

"""## Pearson Correlation
Measure the strength of the correltion between two features

*   Correlation coefficient
*   P-value

Correlation coefficient


*   Close to +1: Large Positive relationship
*   Close to -1: Large Negative relationship
*   Close to 0: No relationship

P-value


*   P-value < 0.001 String certainty in the result
*   P-value < 0.05 Moderate certainty in the result
*   P-value < 0.1 Weak certainty in the result
*   P-value > 0.1 No certainty in the result



Strong Correlation


*   Correlation coefficiente close to 1 or -1
*   P-value less than 0.001
"""

df ["horsepower"]=df ["horsepower"].astype("float")
mean_horse=df ["horsepower"].mean()
df ["horsepower"]=df ["horsepower"].replace(np.nan,mean_horse)



from scipy import stats

Pearson_coef, p_value = stats.pearsonr(df["horsepower"], df["price"])
print("Coeficiente de Pearson: ", Pearson_coef )
print("P-value: ", p_value)

# ANOVA: Finding correlation between different groups of categorical variable
# F-test score: variation between sample group means divided by variation within sample group
# p-value: confidence degree

df[["engine-size", "price"]].corr()

df['drive-wheels'].value_counts().to_frame() # Convierte a dataframe

df['drive-wheels'].unique()

"""<h3>ANOVA: Analysis of Variance</h3>
<p>The Analysis of Variance  (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:</p>

<p><b>F-test score</b>: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.</p>

<p><b>P-value</b>:  P-value tells how statistically significant is our calculated score value.</p>

<p>If our price variable is strongly correlated with the variable we are analyzing, expect ANOVA to return a sizeable F-test score and a small p-value.</p>

<p>Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.</p>

<p>Let's see if different types 'drive-wheels' impact  'price', we group the data.</p>
"""

# ANOVA
# grouping results
df_gptest = df[['drive-wheels','body-style','price']]
grouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()
grouped_test1

grouped_test2=df_gptest[['drive-wheels', 'price']].groupby(['drive-wheels'])
grouped_test2.head(2)
f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  
 
print( "ANOVA results: F=", f_val, ", P =", p_val)

f_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('rwd')['price'])  
   
print( "ANOVA results: F=", f_val, ", P =", p_val)

f_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('fwd')['price'])  
 
print("ANOVA results: F=", f_val, ", P =", p_val)

"""This is a great result, with a large F test score showing a strong correlation and a P value of almost 0 implying almost certain statistical significance. But does this mean all three tested groups are all this highly correlated?

# Model Development
"""

from sklearn.linear_model import LinearRegression
lm=LinearRegression()

X=df[["highway-mpg"]]
Y=df["price"]

lm.fit(X,Y)

Yhat=lm.predict(X)
print("Intercepto: ", lm.intercept_)
print("Coef: ", lm.coef_[0])

plt.plot(X,Y,'ro',X,Yhat,'b-')

"""## Multiple Linear Model Estimator"""

Z=df[["horsepower","curb-weight","engine-size","highway-mpg"]]

Z.head()

lm.fit(Z, df["price"])

Yhat=lm.predict(Z)

lm.coef_

for i, j in zip(Z.columns, lm.coef_):
  print("Coeficiente de la variable ", i, "es: ", j)

print("Intercepto: ", lm.intercept_)

lm.score(Z,Y)

import seaborn as sns
sns.regplot(x="highway-mpg", y="price", data=df)
#plt.ylim(0,)

# RESIDUOS

sns.residplot(df["highway-mpg"], df["price"]) # (VARIABLE INDEPENDIENTE, VARIABLE DEPENDIENTE)

ax1=sns.distplot(df["price"], hist=False, color="red", label="Actual Value")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values", ax=ax1)
# Deben ser muy similares las gr√°ficas



"""# Polynomial Regression"""

from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=2)

x_polly=pr.fit_transform(df[["horsepower", "curb-weight"]], include_bias=False)



"""## Pre-Processing"""

from sklearn.preprocessing import StandardScaler

SCALE=StandardScaler()
SCALE.fit(df[["horsepower", "highway-mpg"]])
x_scale=SCALE.transform(df[["horsepower", "highway-mpg"]])

x_scale



"""# PIPELINES"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Create a list of tuples:

Input=[("scale", StandardScaler()),
       ("polynomial", PolynomialFeatures(degree=2)),
       ("mode", LinearRegression())
       ]

Pipe=Pipeline(Input)   

Pipe.fit(df[["horsepower", "curb-weight", "engine-size", "highway-mpg"]], Y) 
yhat=Pipe.predict(df[["horsepower", "curb-weight", "engine-size", "highway-mpg"]])

yhat



"""## EVALUATION (MEAN SQUARED ERROR - MSE)"""

from sklearn.metrics import mean_squared_error

mean_squared_error(df["price"], Yhat)

# R-squared

#lm.score(X,Yhat)

"""## Prediction"""

lm.fit(df[["highway-mpg"]], df[["price"]])

new_input=np.arange(1,101,1).reshape(-1,1)
lm.predict(new_input)

# from sklearn.model_selection import train_test_split

# x_data: features or independent variables
# y_data: dataset target
# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=0)

"""# CROSS VALIDATION"""

from sklearn.model_selection import cross_val_score

# lr: model
# x_data: predictoras, y_data: variable objetivo
# cv=number of partitions
scores=cross_val_score(lr, x_data, y_data, cv=3)
np.mean(scores)


# cross_val_predict()
# Returns the prediction that was pbtained for each element when it was in the test set

from sklearn.model_selection import cross_val_predict
yhat=cross_val_predict(lr2e, x_data, y_data, cv=3)

"""## Ridge Regression"""

'''
from sklearn.linear_model import Ridge
RigeModel=Ridge(alpha=0.1)
RigeModel.fit(X,y)
Yhat=RigeModel.predict
'''

"""# Grid Search"""

parameters=[{"alpha":[1,10,100,1000]}]

parameters[0]["alpha"]

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

parameters1=[{"alpha":[0.001, 0.1, 10, 100, 1000, 100000, 1000000], "normalize":[True,False]}]

RR=Ridge()
Grid1=GridSearchCV(RR, parameters1, cv=4)
Grid1.fit(df[["horsepower", "curb-weight", "engine-size", "highway-mpg"]], df["price"])

Grid1.best_estimator_

scores=Grid1.cv_results_

scores

scores["mean_test_score"]



for param, mean_val in zip(scores["params"], scores["mean_test_score"]):
  print(param, "R2 on test data: ", mean_val)

scores.keys()

